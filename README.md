# Logistic-Regression--Optimization-Techniques
The goal of this project is to gain a deep understanding of how machine learning models train and optimize their parameters using different gradient-based optimization techniques, implemented manually from scratch

A Logistic Regression model from scratch using only NumPy (no ML libraries).
Support for three optimization algorithms:

1) Gradient Descent (GD)
2) Stochastic Gradient Descent (SGD)
3) Mini-Batch Gradient Descent (MBGD)

Evaluated model performance using:

Confusion Matrix
Accuracy, Precision, Recall, and F1 Score

Plotted the loss over epochs to visually compare the optimizers.
